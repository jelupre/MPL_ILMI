# Задачи классификации

## Оглавление
1. [Метрические алгоритмы классификации](#Метрические-алгоритмы-классификации)
    1. [Метод ближайшего соседа](#Метод-ближайшего-соседа)
    1. [Метод k-ближайших соседей](#Метод-k-ближайших-соседей)
    1. [Метод k-ближайших взвешенных соседей](#Метод-k-ближайших-взвешенных-соседей)
    1. [Преимущества kwNN](#Преимущества-kwNN)
    1. [Метод парзеновского окна](#Метод-парзеновского-окна)
    1. [Метод потенциальных функций](#Метод-потенциальных-функций)
2. [Байесовские методы классификации](#Байесовские-методы-классификации)
    1. [Линии уровня нормального распределения](#Линии-уровня-нормального-распределения)
    2. [Наивный нормальный байесовский классификатор](#Наивный-нормальный-байесовский-классификатор)
    3. [Подстановочный алгоритм](#Подстановочный-алгоритм)
    4. [Линейный дискриминант Фишера](#Линейный-дискриминант-Фишера)
3. [Линейные методы классификации](#Линейные-методы-классификации)
    1. [Стохастический градиентный спуск](#Стохастический-градиентный-спуск)
    2. [Адаптивный линейный элемент](#Адаптивный-линейный-элемент)
    3. [Персептрон Розенблатта](#Персептрон-Розенблатта)
    4. [Логистическая регрессия](#Логистическая-регрессия)
    5. [Сравнение алгоритмов](#Сравнение-алгоритмов)
    6. [Метод опорных векторов](#Метод-опорных-векторов)

## Метрические алгоритмы классификации 

[Оглавление](#Оглавление)

<table>
  <tr>
    <th>Метод</th>
    <th>Параметры</th>
    <th>Точность</th>
  </tr>
  <tr>
    <td>1nn</td>
    <td>k = 1</td>
    <td>0.0467</td>
  </tr>
  <tr>
    <td><strong>knn</strong></td>
    <td><strong>k = 6</strong></td>
    <td><strong>0.0333</strong></td>
  </tr>
  <tr>
    <td>kwnn</td>
    <td>k = 6, q = 0.56</td>
    <td>0.04</td>
  </tr>
  <tr>
    <td>PW (all kernels, except Gauss)</td>
    <td>h = 0.4</td>
    <td>0.04</td>
  </tr>
  <tr>
    <td>PW Gauss</td>
    <td>h = 0.1</td>
    <td>0.04</td>
  </tr>
  <tr>
    <td>MPF (all kernels, except Gauss)</td>
    <td>h = 1</td>
    <td>0.0467</td>
  </tr>
</table>

<p>
  Все метрические алгоритмы классификации будем рассматривать на датасете "Ирисы Фишера", а конкретнее - тренировочная выборка по ширине и длине лепестка и виду ириса. В выборке будет 150 цветков. Вот так, собственно, выглядит наш обучающий набор.
</p>

![set_of_irises](https://github.com/jelupre/ML1/blob/master/images/irises.png)

<p>
  kNN расшифровывается как k Nearest Neighbor или k Ближайших Соседей — это один из самых простых алгоритмов классификации, также иногда используемый в задачах       
  регрессии. Благодаря своей простоте, он является хорошим примером, с которого можно начать знакомство с областью Machine Learning. 
</p>

<p>
  Задача классификации в машинном обучении — это задача отнесения объекта к одному из заранее определенных классов на основании его формализованных признаков. 
  Каждый из объектов в этой задаче представляется в виде вектора в N-мерном пространстве, каждое измерение в котором представляет собой описание одного из признаков 
  объекта. 
</p>